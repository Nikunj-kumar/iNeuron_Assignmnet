{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 Loss(Least Absolute Deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is used to minimize the error, which is a difference of absolute value and predicted value. \n",
    "<br>\n",
    "<br>\n",
    "It will tell you how deviated your data is and not effected by outliers.\n",
    "<br>\n",
    "<br>\n",
    "If we have more positive and negative value, then it will not work better.\n",
    "<br>\n",
    "<br>\n",
    "Formula:\n",
    "<img src=\"Desktop/For/L1.jpeg\" style=\"width:300px;height:200px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 Loss(Least Square Error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also used ot minimize the error, in which we try to get the residual square.\n",
    "<br>\n",
    "<br>\n",
    "If we have outliers then we should not use it as it will give a huge value after squaring them.\n",
    "<br>\n",
    "<br>\n",
    "It provides smoothness and keeps overall error small, if there is no oulier.\n",
    "<br>\n",
    "<br>\n",
    "Formula:\n",
    "\n",
    "<img src=\"Desktop/For/L2.jpeg\" style=\"width:300px;height:200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huber Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huber loss is very less sensitive to the outliers in data than least square error.\n",
    "<br>\n",
    "<br>\n",
    "It is the better than L1 and L2 and overcome their drawbacks.\n",
    "<br>\n",
    "<br>\n",
    "Formula:\n",
    "<img src=\"Desktop/For/Huber.jpeg\" style=\"width:400px;height:200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo-Huber Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a smooth approximation to the Huber loss function.\n",
    "<br>\n",
    "<br>\n",
    "In this you need to take care of \"Delta\", a too big value may make the outliers affect your model again and too small value could make your model very slow to converge to solution.\n",
    "<br>\n",
    "<br>\n",
    "Formula:\n",
    "<img src=\"Desktop/For/pseudo.jpeg\" style=\"width:400px;height:200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hinge Loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a loss function used for training classifiers.\n",
    "<br>\n",
    "<br>\n",
    "It is used for \"maximum-margin\" classification.\n",
    "<br>\n",
    "<br>\n",
    "Higher order hinge loss helps in both speed and final performance\n",
    "<br>\n",
    "<br>\n",
    "Mostly it is used in classification problem.\n",
    "<br>\n",
    "<br>\n",
    "Formula:\n",
    "<img src=\"Desktop/For/hinge.jpeg\" style=\"width:400px;height:200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Squared Hinge Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a loss function used for \"maximum margin\" binary classification.\n",
    "<br>\n",
    "<br>\n",
    "It is used in problem involving yes/no desicion and when you are not interested in knowing how certain the classifier is about classification.\n",
    "<br>\n",
    "<br>\n",
    "Formula: Square of output of the hinge's max(0, 1 - t*y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catergorical Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also known as Softmax Loss. It is a Softmax activation function plus a Cross-Entropy loss. It is used for Multi-Class Classification.\n",
    "<br>\n",
    "<br>\n",
    "If we use this loss, we will trian a CNN to output probability over the C classes for each image.\n",
    "<br>\n",
    "<br>\n",
    "It makes gradient bit more complex, since the loss contains an element for each positive class.\n",
    "<br>\n",
    "<br>\n",
    "Formula:\n",
    "<img src=\"Desktop/For/Cat_entropy.jpeg\" style=\"width:400px;height:200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also known as Sigmoid Cross Entropy loss. It is a sigmoid activatation function plus a cross entropy loss. \n",
    "<br>\n",
    "<br>\n",
    "Unlinke Softmax loss, it is independent for each vector component, meaning that the loss computed for every CNN output vector component is not effected by other componenet values.\n",
    "<br>\n",
    "<br>\n",
    "Formula:\n",
    "<img src=\"Desktop/For/binary_entropy.jpeg\" style=\"width:400px;height:200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focal Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also a type of Cross-Entropy loss that weighs the contribution of each sample to the loss based in classification error.\n",
    "<br>\n",
    "<br>\n",
    "If a sample is already classified correctlyu by CNN, its contribute to loss decrease, this strategy is used to solve the problem of class imbalance.\n",
    "<br>\n",
    "<br>\n",
    "In it gradient get bit more complex due to the inclusion of the modulating factor in loss formulation.\n",
    "<br>\n",
    "<br>\n",
    "Formula:\n",
    "<img src=\"Desktop/For/focal_loss.jpeg\" style=\"width:400px;height:200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kullback-Leibler Divergence Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a measure of how one probability distribution is different from second, refernce probability distribution.\n",
    "\n",
    "Formula:\n",
    "<img src=\"Desktop/For/kull.jpeg\" style=\"width:400px;height:200px\"/>\n",
    "\n",
    "If we think of above formula it interpret this as \"how many bits of information we expect to lose\".\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
