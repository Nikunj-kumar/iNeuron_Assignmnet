{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Desktop/Activation_function_logo/blank.jpg\" style=\"width:50px;height:50px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid function gives an \"S\" shaped curve. The function maps any real value into another value between 0 and 1.\n",
    "\n",
    "Equation: f(x) = 1/(1+$e^{-x}$)\n",
    "\n",
    "Range: (0,1)\n",
    "\n",
    "Equation Graph \n",
    "\n",
    "<img src=\"Desktop/Activation_function_logo/sigmoid.jpg\" style=\"width:200px;height:200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivative: f'(x) = (1 / 1+$e^{-x}$) * (1 - (1 / (1+$e^{-x}$))\n",
    "\n",
    "Derivative graph\n",
    "\n",
    "<img src=\"Desktop/Activation_function_logo/sigmoid_der.jpg\" style=\"width:200px;height:200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantage :-\n",
    "1. The function is differentiable, means we can find slope of the sigmoid curve at any two points.\n",
    "2. Output values is between 0 and 1, normalizing the output of each neuron.\n",
    "\n",
    "\n",
    "\n",
    "Disadvantage:-\n",
    "1. It has vanishing gradient, for very high or very low value of X, there is almost no change in prediction.\n",
    "2. Output is not zero centered\n",
    "3. Computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Desktop/Activation_function_logo/blank.jpg\" style=\"width:50px;height:50px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tanh / Hyperbolic tangent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanh is very similar to sigmoid function, only difference is it is symmetric around the origin. The range of values in this case is in from -1 to 1. Hence input to the next layers will not always be of the same sign.\n",
    "\n",
    "Equation: f(x) = n = tanh(x) = ($e^{x}$ - $e^{-x}$) / ($e^{x}$ + $e^{-x}$)\n",
    "\n",
    "Range: (-1,1)\n",
    "\n",
    "Equation Graph\n",
    "\n",
    "<img src=\"Desktop/Activation_function_logo/tanh.jpg\" style=\"width:200px;height:200px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivative: f'(x) = (1 - $n^{2}$)\n",
    "\n",
    "Derivative graph:\n",
    "\n",
    "<img src=\"Desktop/Activation_function_logo/tanh_der.jpg\" style=\"width:200px;height:200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantage:\n",
    "1. It is zero centered means it make easier for model that have strongly negative, neutral and strongly positive values.\n",
    "2. It works better than sigmoid function\n",
    "\n",
    "Disadvantage:\n",
    "1. It has also vanishing gradient issue.\n",
    "2. It has also slow convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Desktop/Activation_function_logo/blank.jpg\" style=\"width:50px;height:50px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU(Rectified Linear Unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is one of the most common activation function, it's main advantage is that it doesn't activate all the neurons at the same time, means neurons will be deactivated if the output o linear transformation is less than 0.\n",
    "\n",
    "Equation: f(x) = max(0,x)\n",
    "\n",
    "Range: (0,+${\\infty}$)\n",
    "\n",
    "Equation Graph:\n",
    "\n",
    "<img src=\"Desktop/Activation_function_logo/relu.jpg\" style=\"width:200px;height:200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivative: \n",
    "<br>\n",
    "f'(x) = 1,if x>=0\n",
    "<br>\n",
    "        0, if x<0\n",
    "                    \n",
    "Derivative Graph:\n",
    "\n",
    "<img src=\"Desktop/Activation_function_logo/relu_der.jpg\" style=\"width:200px;height:200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantage:\n",
    "1. Computationally efficient, allows the network to converge very quickly.\n",
    "2. It has a derivative function and allows for back-propagation.\n",
    "\n",
    "Disadvantage:\n",
    "1. When inputs approach zero or negative, the gradient of the function becomes zero.\n",
    "2. For negative value, network cannot perform back-propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Desktop/Activation_function_logo/blank.jpg\" style=\"width:50px;height:50px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is one of the attempt to fix the dying ReLU problem. Leaky units are the ones that have a very small gradient instead of a zero gradient when the input is negative, giving the chance for the network to continue its learning.\n",
    "\n",
    "Equation: f(x) = max(0.01x, x)\n",
    "\n",
    "Range: (0.01, +${\\infty}$)\n",
    "\n",
    "Eqution graph:\n",
    "\n",
    "<img src=\"Desktop/Activation_function_logo/lrelu.jpg\" style=\"width:300px;height:200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivative: \n",
    "<br>\n",
    "f'(x) = 0.01  if x<0\n",
    "<br>\n",
    "        1    otherwise\n",
    "                     \n",
    "Derivative Graph:\n",
    "\n",
    "<img src=\"Desktop/Activation_function_logo/lrelu_der.jpg\" style=\"width:200px;height:200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantage:\n",
    "1. This variation of ReLU has a small positive slope in negative area, so it does enable back-propagation, even for negative input values.\n",
    "\n",
    "Disadvantage\n",
    "1. It's doen't provide consistent prediction for negative input values.\n",
    "2. In forward propagation if the learning rate is very high it will overshoot killing the neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Desktop/Activation_function_logo/blank.jpg\" style=\"width:50px;height:50px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponential Linear Unit (ELU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very similar to ReLU except negative inputs. ELU becomes smooth slowly until its output equal to -a, whereas ReLU sharply smoothes. It tend to converge cost to zero faster and produce more accurate results.\n",
    "\n",
    "Equation :   \n",
    "<br>\n",
    "f(x) = x            if x > 0\n",
    "<br>\n",
    "a.($e^{x}$  - 1)  if x < 0\n",
    "\n",
    "Equation Graph:\n",
    "\n",
    "<img src=\"Desktop/Activation_function_logo/ELU.jpg\" style=\"width:200px;height:200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivative:\n",
    "<br>\n",
    "f'(x) = 1  if x > 0\n",
    "<br>\n",
    "(a.($e^{x}$  - 1) + a) if x <= 0\n",
    "\n",
    "Derivative Equation:\n",
    "<img src=\"Desktop/Activation_function_logo/elu_der.jpg\" style=\"width:200px;height:200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantage\n",
    "1. Avoids the dead relu problem and produces negtive output. which helps the network nudge the weight and biases in the right direction.\n",
    "2. Produce activation instead of letting them be zero, when calculating the gradient.\n",
    "\n",
    "Disadvantage \n",
    "1. Introduces longer computation time, because of exponential operation included.\n",
    "2. It does not avoid the exploding gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Desktop/Activation_function_logo/blank.jpg\" style=\"width:50px;height:50px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a straight line function, where activation is proportinal to input(which is weighted sum from neuron)\n",
    "\n",
    "Equation: f(z,m) = {z * m}\n",
    "\n",
    "Equation Graph:\n",
    "<img src=\"Desktop/Activation_function_logo/linear.jpg\" style=\"width:200px;height:200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivative: f'(x) = {m}\n",
    "\n",
    "Derivative Graph:\n",
    "\n",
    "<img src=\"Desktop/Activation_function_logo/linear_der.jpg\" style=\"width:200px;height:200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantage:\n",
    "1. It gives a range of activation, so it is not binary activation.\n",
    "2. We can connect few neurons together and if more than 1 fires, we could take the max and decide based on that.\n",
    "\n",
    "Disadvantage:\n",
    "1. Derivative is constant, so gradient has no relationship with x.\n",
    "2. If there is an error in prediction, the changes made by back prapogation is constant and not depending on the chnage in input delta(x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Desktop/Activation_function_logo/blank.jpg\" style=\"width:50px;height:50px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is formulated as x times sigmoid x. Since ReLU produces 0 output for negative inputs and it can not be back-propagated. Herein, swish can partially handle this problem.\n",
    "\n",
    "Equation: f(x) = x / (1+$e^{-x}$) \n",
    "<br>\n",
    "y = x * delta(x)\n",
    "\n",
    "Equation Graph:\n",
    "\n",
    "<img src=\"Desktop/Activation_function_logo/swish.jpg\" style=\"width:300px;height:200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivative: y' = x' * delta(x) + x * delta(x)'\n",
    "\n",
    "Derivative Graph:\n",
    "\n",
    "<img src=\"Desktop/Activation_function_logo/swish_der.jpg\" style=\"width:300px;height:200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantage:\n",
    "1. It can handle vanishing gradient problem.\n",
    "2. It works better than ReLU till some extent.\n",
    "\n",
    "Disadvantage\n",
    "1. Compuatation is too much high for both feed forwarding and back propogation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Desktop/Activation_function_logo/blank.jpg\" style=\"width:50px;height:50px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softplus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a newer function than tanh and sigmoid. It is an alternative of these traditional functions because it is differentiable\n",
    "and it's derivative is easy to demonstrate. \n",
    "Output produced by sigmoid and tanh has upper and lower limit, where as softplus function produces output in scale of (0, +${\\infty}$).\n",
    "\n",
    "Equation: f(x) = ln(1+${e^x}$)\n",
    "\n",
    "Equation Graph:\n",
    "<img src=\"Desktop/Activation_function_logo/softplus.jpg\" style=\"width:300px;height:200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivative: f'(x) = 1/(1+$e^{-x}$)\n",
    "\n",
    "Derivative Graph:\n",
    "<img src=\"Desktop/Activation_function_logo/softplus_der.jpg\" style=\"width:300px;height:200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantage:\n",
    "1. This function is much more smoother.\n",
    "2. It is unilaterally supressed, and has a wide acceptance domain.\n",
    "\n",
    "Disadvantage:\n",
    "1. Due to logrithm operation, it is compuattionaly intensivea and is not used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Desktop/Activation_function_logo/blank.jpg\" style=\"width:50px;height:50px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maxout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It selects the maximum of the inputs. It enjoys all benefit of ReLU and it doesn't have it's drawback(dying ReLU).\n",
    "\n",
    "Equation: max(w1x1 + b1, w2x2 + b2)\n",
    "\n",
    "Equation Graph:\n",
    "<img src=\"Desktop/Activation_function_logo/max.jpg\" style=\"width:300px;height:200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantage:\n",
    "1. It's ability to fit is very strong and can fit any convex function\n",
    "2. It has advantage of linearity and unsaturation.\n",
    "\n",
    "Disadvantage:\n",
    "1. There are two set of parameter in each neuron, then the parameter quantity is doubled , which surges the number of overall\n",
    "    parameter.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Desktop/Activation_function_logo/blank.jpg\" style=\"width:50px;height:50px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It calculates the probabilities distribution of event over \"n\" different event. \n",
    "\n",
    "Equation : f(x) = g(x)/h(x) = eˣᵢ / (Σⱼ₌₀ eˣᵢ)\n",
    "    \n",
    "Range: (0,1)\n",
    "\n",
    "Equation Graph:\n",
    "<img src=\"Desktop/Activation_function_logo/softmax.jpg\" style=\"width:300px;height:200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivative: f'(x) = g'(x)h(x) - h'(x)g(x) / ${[h(x)]^2}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantage:\n",
    "1. It has ability to handle multiple class.\n",
    "2. It is useful for output neurons, typically it is used only at output layer, for neural network that need to classify input\n",
    "   into multiple categories.\n",
    "   \n",
    "Disadvantage:\n",
    "1. It doesn't support null rejection.\n",
    "2. It will not work if your data is not linearly separable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
